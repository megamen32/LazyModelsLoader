# LazyModels

**LazyModels** is a lightweight, zero-hassle utility for lazy-loading HuggingFace Transformers **and** SentenceTransformer models with automatic memory management on **CPU and GPU**.

### âœ… Install

```bash
pip install lazymodels
```

---

## ğŸš€ Features

* ğŸ”„ **Lazy loading** of models by name (nothing is loaded until you call it)
* ğŸ§  **Smart task detection**: causal-lm, seq2seq, token classification, etc.
* ğŸ“¦ Supports **HuggingFace Transformers** *and* **SentenceTransformers**
* ğŸ§¼ **Auto-unloading** of older models when memory limit is hit
* ğŸ§  **Device-aware** (CPU/GPU auto-detection)
* âš¡ Works in **1 line of code**
* 

---

## ğŸ§ª Example

```python
from lazymodels import lazy_model_transformer

# Automatically loads a SentenceTransformer lazily
model = lazy_model_transformer("paraphrase-multilingual-MiniLM-L12-v2")

# Or a Transformers causal LM
gpt = lazy_model_transformer("gpt2")

# With tokenizer if needed
model, tokenizer = lazy_model_transformer("t5-small", tokenizer=True)
```

You can also access the global memory manager:

```python
from lazymodels import get_lazy_model_manager

print(get_lazy_model_manager().list_loaded())
```

---

## ğŸ’» Memory Management

* Automatically detects available memory (CPU or GPU)
* Unloads oldest model when limit is exceeded
* If you don't specify memory limit, the manager uses **available memory at launch** (not full installed memory!)

---

## ğŸ›  Advanced Usage

```python
from lazymodels import get_lazy_model_manager
manager = get_lazy_model_manager()

# Manually register any custom loader
manager.register(
    name="custom::model",
    loader=lambda: YourModelClass(...),
    imports=["your_model_lib"]
)

# Load by name later
model = manager.get("custom::model")
```

---

## ğŸ“¦ Dependencies

* `transformers`
* `torch`
* `sentence-transformers`
* `psutil`

---

## ğŸ§  Tip

Perfect for prototyping, memory-sensitive scripts, and large-model inference workflows.

---

MIT licensed. Made with â¤ï¸ and pragmatism.
